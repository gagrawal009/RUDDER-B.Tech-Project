{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from matplotlib import pyplot as plt\n",
    "from widis_lstm_tools_master.widis_lstm_tools.nn import LSTMLayer\n",
    "\n",
    "# Prepare some random generators for later\n",
    "rnd_gen = np.random.RandomState(seed=123)\n",
    "_ = torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(Dataset):\n",
    "    def __init__(self, n_samples: int, max_timestep: int, n_positions: int, rnd_gen: np.random.RandomState):\n",
    "        \"\"\"Our simple 1D environment as PyTorch Dataset\"\"\"\n",
    "        super(Environment, self).__init__()\n",
    "        n_actions = 2\n",
    "        zero_position = int(np.ceil(n_positions / 2.))\n",
    "        coin_position = zero_position + 2\n",
    "        \n",
    "        # Generate random action sequences\n",
    "        actions = np.asarray(rnd_gen.randint(low=0, high=2, size=(n_samples, max_timestep)), dtype=np.int)\n",
    "        actions_onehot = np.identity(n_actions, dtype=np.float32)[actions]\n",
    "        \n",
    "        # Generate observations from action sequences\n",
    "        actions[:] = (actions * 2) - 1\n",
    "        observations = np.full(fill_value=zero_position, shape=(n_samples, max_timestep), dtype=np.int)\n",
    "        for t in range(max_timestep-1):\n",
    "            action = actions[:, t]\n",
    "            observations[:, t+1] = np.clip(observations[:, t] + action, 0, n_positions-1)\n",
    "        observations_onehot = np.identity(n_positions, dtype=np.float32)[observations]\n",
    "        \n",
    "        # Calculate rewards (sum over coin position for all timesteps)\n",
    "        rewards = np.zeros(shape=(n_samples, max_timestep), dtype=np.float32)\n",
    "        rewards[:, -1] = observations_onehot[:, :, coin_position].sum(axis=1)\n",
    "        \n",
    "        self.actions = actions_onehot\n",
    "        self.observations = observations_onehot\n",
    "        self.rewards = rewards\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.rewards.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.observations[idx], self.actions[idx], self.rewards[idx]\n",
    "\n",
    "\n",
    "n_positions = 13\n",
    "env = Environment(n_samples=1000, max_timestep=50, n_positions=13, rnd_gen=rnd_gen)\n",
    "env_loader = torch.utils.data.DataLoader(env, batch_size=8, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "obs0, a0, r0 = env.__getitem__(3)\n",
    "obs1, a1, r1 = env.__getitem__(25)\n",
    "fig, axes = plt.subplots(3, 2, figsize=(8, 4.5), dpi=100)\n",
    "axes[0, 0].plot(obs0.argmax(-1) - 6)\n",
    "axes[0, 1].plot(obs1.argmax(-1) - 6)\n",
    "axes[0, 0].set_ylim(-6, 6)\n",
    "axes[0, 1].set_ylim(-6, 6)\n",
    "axes[0, 0].axhline(2, linestyle='--', color='r')\n",
    "axes[0, 1].axhline(2, linestyle='--', color='r')\n",
    "axes[0, 0].xaxis.grid(True)\n",
    "axes[0, 1].xaxis.grid(True)\n",
    "axes[0, 0].set_title('observations (sample 1)')\n",
    "axes[0, 1].set_title('observations (sample 2)')\n",
    "axes[0, 0].set_xlabel('time (environment steps)')\n",
    "axes[0, 1].set_xlabel('time (environment steps)')\n",
    "\n",
    "axes[1, 0].plot(a0.argmax(-1))\n",
    "axes[1, 1].plot(a1.argmax(-1))\n",
    "axes[1, 0].xaxis.grid(True)\n",
    "axes[1, 1].xaxis.grid(True)\n",
    "axes[1, 0].set_title('actions (sample 1)')\n",
    "axes[1, 1].set_title('actions (sample 2)')\n",
    "axes[1, 0].set_xlabel('time (environment steps)')\n",
    "axes[1, 1].set_xlabel('time (environment steps)')\n",
    "\n",
    "axes[2, 0].plot(r0)\n",
    "axes[2, 1].plot(r1)\n",
    "axes[2, 0].xaxis.grid(True)\n",
    "axes[2, 1].xaxis.grid(True)\n",
    "axes[2, 0].set_title('original rewards (sample 1)')\n",
    "axes[2, 1].set_title('original rewards (sample 2)')\n",
    "axes[2, 0].set_xlabel('time (environment steps)')\n",
    "axes[2, 1].set_xlabel('time (environment steps)')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_positions, n_actions, n_lstm):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # This will create an LSTM layer where we will feed the concatenate\n",
    "        self.lstm1 = LSTMLayer(\n",
    "            in_features=n_positions+n_actions, out_features=n_lstm, inputformat='NLC',\n",
    "            # cell input: initialize weights to forward inputs with xavier, disable connections to recurrent inputs\n",
    "            w_ci=(torch.nn.init.xavier_normal_, False),\n",
    "            # input gate: disable connections to forward inputs, initialize weights to recurrent inputs with xavier\n",
    "            w_ig=(False, torch.nn.init.xavier_normal_),\n",
    "            # output gate: disable all connection (=no forget gate) and disable bias\n",
    "            w_og=False, b_og=False,\n",
    "            # forget gate: disable all connection (=no forget gate) and disable bias\n",
    "            w_fg=False, b_fg=False,\n",
    "            # LSTM output activation is set to identity function\n",
    "            a_out=lambda x: x\n",
    "        )\n",
    "        \n",
    "        # After the LSTM layer, we add a fully connected output layer\n",
    "        self.fc_out = torch.nn.Linear(n_lstm, 1)\n",
    "    \n",
    "    def forward(self, observations, actions):\n",
    "        # Process input sequence by LSTM\n",
    "        lstm_out, *_ = self.lstm1(torch.cat([observations, actions], dim=-1),\n",
    "                                  return_all_seq_pos=True  # return predictions for all sequence positions\n",
    "                                  )\n",
    "        net_out = self.fc_out(lstm_out)\n",
    "        return net_out\n",
    "\n",
    "\n",
    "# Create Network\n",
    "device = 'cpu'\n",
    "net = Net(n_positions=n_positions, n_actions=2, n_lstm=16)\n",
    "_ = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossfunction(predictions, rewards):\n",
    "    returns = rewards.sum(dim=1)\n",
    "    # Main task: predicting return at last timestep\n",
    "    main_loss = torch.mean(predictions[:, -1] - returns) ** 2\n",
    "    # Auxiliary task: predicting final return at every timestep ([..., None] is for correct broadcasting)\n",
    "    aux_loss = torch.mean(predictions[:, :] - returns[..., None]) ** 2\n",
    "    # Combine losses\n",
    "    loss = main_loss + aux_loss * 0.5\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "update = 0\n",
    "n_updates = 10000\n",
    "running_loss = 100.\n",
    "progressbar = tqdm.tqdm(total=n_updates)\n",
    "while update < n_updates:\n",
    "    for data in env_loader:\n",
    "        # Get samples\n",
    "        observations, actions, rewards = data\n",
    "        observations, actions, rewards = observations.to(device), actions.to(device), rewards.to(device)\n",
    "        \n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Get outputs for network\n",
    "        outputs = net(observations=observations, actions=actions)\n",
    "        \n",
    "        # Calculate loss, do backward pass, and update\n",
    "        loss = lossfunction(outputs[..., 0], rewards)\n",
    "        loss.backward()\n",
    "        running_loss = running_loss*0.99 + loss*0.01\n",
    "        optimizer.step()\n",
    "        update += 1\n",
    "        progressbar.set_description(f\"Loss: {running_loss:8.4f}\")\n",
    "        progressbar.update(1)\n",
    "\n",
    "progressbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
